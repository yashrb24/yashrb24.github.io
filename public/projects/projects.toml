[[collection]]
type = "art"
title = "Graph Neural Networks for Sign Language Recognition"
subtitle = "Supervisor: Dr. Sougata Sen @ SenseLab, BITS Goa"
link = "https://github.com/Mobile-and-Wearable-Sensing-Lab/isl-gnn"
img = "/img/stgcn-pipeline.jpeg"
content = """
Real-time detection of 263 classes from the INCLUDE dataset on resource-constrained edge devices. Experimented with BiLSTMs and Spatio-Temporal Graph Convolutional Networks to replace the heavy MobileNet encoders used in their paper. Achieved ~88% accuracy while significantly reducing parameter count. Currently integrating the model into a mobile app; preparing for conference submission
"""

[[collection]]
type = "art"
title = "CountCLIP: Reproducibility Study"
subtitle = "ReScience C Submission. Work done @ SAiDL"
link = "https://github.com/SforAiDl/CountCLIP"
img = "/img/countclip.gif"
content = """
Standard VLMs like CLIP struggle to differentiate between correct and incorrect object counts. Conducted a rigorous reproducibility study of the "CountCLIP" method and proposed a Multi-Class Counting Loss and adaptive hyperparameter tuning to mitigate overfitting on frequent classes. Achieved 28.88% accuracy (vs 27.5% baseline) on the CountBench benchmark. Released cleaned codebase and repaired evaluation benchmark to the open-source community.
"""

[[collection]]
type = "card"
title = "Dynamic File Striping Configuration for HPC Storage"
subtitle = "Supervisor: Dr. Arnab K Paul @ DashLab, BITS Goa"
content = """
Default striping configurations in parallel file systems lead to significant I/O performance drops and imbalanced data distribution across storage targets. We built a cluster-aware adaptive framework that analyzes historical application logs (e.g., Darshan) to derive optimal striping parameters. We use a syscall interceptor that captures runtime file operations and dynamically enforces the calculated striping configurations via the striping API provided by the file system.. Code out soon!
"""

[[collection]]
type = "art"
title = "University Rover Team"
subtitle = "Project Kratos, Software Team Member"
link = "https://youtu.be/3rKJc1q7uPE?si=lzGFu3drF5zAECgA&t=193"
img = "/img/kratos.jpg"
content = """
Developed a monocular visual servoing system for autonomous navigation using a custom trained YOLOv3 for object detection. Used A* algorithm to determine the path to the target location using depth heatmaps from Zed2i camera. Implemented a PID-controlled GPS navigation system using RTK, enabling precise rover traversal between nearby local coordinates. We stood second in the Autonomous task at International Rover Challenge 2023!
"""
